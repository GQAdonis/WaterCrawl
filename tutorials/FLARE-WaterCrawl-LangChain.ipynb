{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f0b9afa",
   "metadata": {},
   "source": [
    "### üöÄ WaterCrawl¬†√ó¬†FLARE ‚Äî the perfect duo for RAG playgrounds!\n",
    "\n",
    "Welcome to this **step‚Äëby‚Äëstep Jupyter¬†Notebook** where we:\n",
    "\n",
    "1. üï∑ **Crawl & clean** any website with **WaterCrawl** ‚Äì turning raw HTML into markdown/JSON that‚Äôs ready for embeddings.¬†  \n",
    "2. üîç **Retrieve on‚Äëthe‚Äëfly** with **FLARE (Forward‚ÄëLooking¬†Active¬†REtrieval)** ‚Äì an ‚Äúalways‚Äëbe‚Äëfact‚Äëchecking‚Äù wrapper that pulls extra docs *only* when the LLM shows low confidence.  \n",
    "3. üõ† **Tie it all together** with **LangChain**, **Tavily Search¬†API** & a few helper utils so you can remix the pipeline to your heart‚Äôs content.\n",
    "\n",
    "---\n",
    "\n",
    "#### What‚Äôs inside?\n",
    "\n",
    "| üîß Component | üí° Why we‚Äôre using it |\n",
    "|--------------|----------------------|\n",
    "| **WaterCrawl** | Point‚Äë&‚Äëshoot crawling with sitemap visualizer, duplicate detection, and markdown/JSON exports ‚Äì perfect for vector DB ingestion. :contentReference[oaicite:0]{index=0} |\n",
    "| **LangChain** | Glue layer that lets us chain the crawl ‚Üí embed ‚Üí FLARE retrieval steps with a few lines of code. |\n",
    "| **Tavily¬†Search¬†API** | Fast, inexpensive web search that slots into `TavilyRetriever`; great complement to your own crawled corpora. |\n",
    "| **FLARE** | Re‚Äëchecks the model‚Äôs ‚Äúnext sentence‚Äù for shaky tokens; if confidence is low, it auto‚Äëgenerates a smart query and fetches fresh docs before writing. :contentReference[oaicite:1]{index=1} |\n",
    "\n",
    "---\n",
    "\n",
    "#### Notebook flow üó∫Ô∏è\n",
    "\n",
    "1. **Setup**: grab your API keys from https://watercrawl.dev/, spin up your own `watercrawl` from: https://github.com/watercrawl/watercrawl. To run WaterCrawl API you need to install the Python SDK, which we will do in the following steps\n",
    "2. **FLARE chain**: initialize `FlareChain(llm_answer, llm_question, retriever)` with **Tavily** + your newly‚Äëminted vector store.  \n",
    "3. **Ask away!**: watch FLARE pause, retrieve, and resume writing‚Äîas many times as needed‚Äîto give rock‚Äësolid answers.  \n",
    "4. **Extras**: show off the visual sitemap PNG WaterCrawl generated and link each node to its vector IDs.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why you‚Äôll ‚ù§Ô∏è this combo\n",
    "\n",
    "- **Less hallucination, more citation**: WaterCrawl hands FLARE pristine, source‚Äëmapped text, so every sentence can be traced back to a URL.  \n",
    "- **Pay only for what you need**: FLARE calls Tavily *selectively*, not on every token‚Äîso your search bill stays tiny.  \n",
    "- **Drop‚Äëin for any stack**: swap Tavily for your own BM25/Elastic/Weaviate retriever, or point WaterCrawl at authenticated intranet sites.  \n",
    "- **Open‚Äësource all the way**: MIT‚Äëstyle licences on both projects mean you can fork, tweak, and ship to prod. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "> **Tip:** if you‚Äôre new to WaterCrawl, follow: https://github.com/watercrawl/watercrawl?tab=readme-ov-file#-quick-start  hit `http://localhost` after `docker compose up -d` and explore the Playground UI‚Äîselector testing & screenshot capture included! üé®\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Let‚Äôs spin up containers and start crawling! üèÅ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e41fe0",
   "metadata": {},
   "source": [
    "##### ‚û°Ô∏è **Lets install üì¶all the dependencies:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f902f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install  langchain-community langchain-core langchain-openai notebook watercrawl-py tavily-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bbe7f",
   "metadata": {},
   "source": [
    "### ‚û°Ô∏è üîë **API keys you‚Äôll need (grab these first!)** \n",
    "\n",
    "| Service | What it‚Äôs for | Where to generate |\n",
    "|---------|---------------|-------------------|\n",
    "| **WaterCrawl** | Auth for crawling endpoints | <https://app.watercrawl.dev/dashboard/api-keys> |\n",
    "| **OpenAI** | LLM + embeddings | <https://platform.openai.com/api-keys> |\n",
    "| **Tavily Search** | Web search for FLARE | <https://app.tavily.com/home> |\n",
    "\n",
    "---\n",
    "\n",
    "Option¬†1 ‚Äìkeep it clean: use a `.env` file ‚ö†Ô∏è\n",
    "\n",
    "\n",
    "Create the file **once**, store your keys, and everything else ‚Äújust works‚Äù.\n",
    "\n",
    "```python\n",
    "# ‚îÄ‚îÄ create_env.py ‚îÄ‚îÄ\n",
    "env_text = \"\"\"\n",
    "OPENAI_API_KEY= ***put your APi key here *** \n",
    "TAVILY_API_KEY= ***put your APi key here *** \n",
    "WATERCRAWL_API_KEY=* **put your APi key here *** \n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_text)\n",
    "print(\".env file created ‚Äî now edit it with your real keys ‚úèÔ∏è\")\n",
    "\n",
    "-------------------------------------------------\n",
    "Option¬†2 ‚Äì quick‚Äëand‚Äëdirty: hard‚Äëcode in the notebook ‚ö†Ô∏è\n",
    "\n",
    "OPENAI_API_KEY= ***put your APi key here *** \n",
    "TAVILY_API_KEY= ***put your APi key here *** \n",
    "WATERCRAWL_API_KEY=* **put your APi key here *** \n",
    "\n",
    "Not recommended ‚Äî anyone who sees or commits the notebook can read your keys.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac82939",
   "metadata": {},
   "source": [
    "##### ‚û°Ô∏è **If you‚Äôre using a¬†`.env`¬†file load the API keys with dotenv** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1babea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # pulls everything from .env\n",
    "\n",
    "OPENAI_API_KEY   = os.environ.get(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY   = os.environ.get(\"TAVILY_API_KEY\")\n",
    "WATERCRAWL_API_KEY = os.environ.get(\"WATERCRAWL_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7e4b63d",
   "metadata": {},
   "source": [
    "##### ‚û°Ô∏è **Import our packages**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7888f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForRetrieverRun,\n",
    "    CallbackManagerForRetrieverRun,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from watercrawl import WaterCrawlAPIClient\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import requests\n",
    "from langchain.chains import FlareChain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f552dce",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è **Lets build WaterCrawl Retriever**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43a04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_tool(query: str, api_key: str, max_results: int = 3) -> List[str]:\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"topic\": \"general\",\n",
    "        \"search_depth\": \"basic\",\n",
    "        \"max_results\": max_results,\n",
    "        \"include_answer\": False,\n",
    "        \"include_raw_content\": False,\n",
    "        \"include_domains\": [],\n",
    "        \"exclude_domains\": []\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "    print([item.get(\"url\") for item in results.get(\"results\", []) if item.get(\"url\")])\n",
    "    return [item.get(\"url\") for item in results.get(\"results\", []) if item.get(\"url\")]\n",
    "\n",
    "\n",
    "class WaterCrawlRetriever(BaseRetriever, BaseModel):\n",
    "    client: WaterCrawlAPIClient\n",
    "    tavily_api_key: str\n",
    "    page_options: dict = {\n",
    "        \"exclude_tags\": [\"nav\", \"footer\", \"aside\"],\n",
    "        \"include_tags\": [\"article\", \"main\"],\n",
    "        \"wait_time\": 100,\n",
    "        \"include_html\": False,\n",
    "        \"only_main_content\": True,\n",
    "        \"include_links\": False\n",
    "    }\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: Any\n",
    "    ) -> List[Document]:\n",
    "        documents = []\n",
    "        try:\n",
    "            urls = search_tool(query, self.tavily_api_key, max_results=3)\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    result = self.client.scrape_url(url=url, page_options=self.page_options, sync=True, download=True)\n",
    "                    content = result.get(\"content\", \"\")\n",
    "                    if content:\n",
    "                        documents.append(Document(page_content=content, metadata={\"source\": url}))\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to fetch content from {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Tavily search failed: {e}\")\n",
    "        return documents\n",
    "\n",
    "    async def _aget_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForRetrieverRun,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[Document]:\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070d91e",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è **Create the langchain retriever obect using WaterCrawlRetriever we have built above**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c7d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = WaterCrawlRetriever(client=WaterCrawlAPIClient(api_key=WATERCRAWL_API_KEY),\n",
    "    tavily_api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92478194",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è **FLARE Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577e7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set this so we can see what exactly is going on\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300d783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( model=\"gpt-4o\", temperature=0)\n",
    "flare = FlareChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    max_generation_len=164,\n",
    "    min_prob=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f3d5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain what is watercrawl tool and how I can improve the LLM performance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1bfa8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What type of software is the Watercrawl tool?', 'How can you optimize the data extraction process to improve the LLM performance?']\u001b[0m\n",
      "['https://github.com/watercrawl/watercrawl', 'https://docs.watercrawl.dev/intro', 'https://watercrawl.dev/']\n",
      "['https://techcommunity.microsoft.com/blog/azure-ai-services-blog/maximizing-data-extraction-precision-with-dual-llms-integration-and-human-in-the/4236728', 'https://medium.com/intel-tech/four-data-cleaning-techniques-to-improve-large-language-model-llm-performance-77bee9003625', 'https://www.turing.com/resources/understanding-data-processing-techniques-for-llms']\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response:  The Watercrawl tool is a software tool used for web crawling and data extraction. It helps in collecting data from websites for various purposes such as research, analysis, or monitoring. To improve the LLM (Large Language Model) performance, you can consider optimizing the data collection process using Watercrawl to ensure high-quality and relevant data inputs for training the model. Additionally, you can also focus on fine-tuning the hyperparameters of the LLM and increasing the training data size to enhance its performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'Explain what is watercrawl tool and how I can improve the LLM performance?',\n",
       " 'response': 'The Watercrawl tool is a software tool used for web crawling and data extraction. It helps in collecting data from websites for various purposes such as research, analysis, or monitoring. To improve the LLM (Large Language Model) performance, you can consider optimizing the data collection process using Watercrawl to ensure high-quality and relevant data inputs for training the model. Additionally, you can also focus on fine-tuning the hyperparameters of the LLM and increasing the training data size to enhance its performance. '}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flare.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è **Now lets see a simple Open AI chain so we can see the value of the FLARE Chain**\n",
    "#### for the test query we provided, the answer of the same LLM is completely wrong!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bed8944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWatercrawl is a web performance testing tool that helps in analyzing the load and stress on a website or web application. It simulates real-world user traffic and measures the website's response time, throughput, and server performance under different load conditions.\\n\\nTo improve the LLM (Load, Latency, and Memory) performance using Watercrawl, the following steps can be taken:\\n\\n1. Identify bottlenecks: Watercrawl helps in identifying the areas of the website that are causing performance issues. It provides detailed reports on page load times, HTTP requests, and server response times, which can help in identifying the bottlenecks.\\n\\n2. Optimize website code: Based on the reports generated by Watercrawl, developers can optimize the website's code to reduce page load times and improve server response times. This can include techniques like minimizing HTTP requests, optimizing images, and using caching mechanisms.\\n\\n3. Test under different load conditions: Watercrawl allows testing under different load conditions, such as low, medium, and high traffic. This helps in identifying how the website performs under different levels of user traffic and if it can handle a sudden surge in traffic.\\n\\n4. Test from different geographical locations: Watercrawl offers the option to test the website's performance from different geographical locations. This helps in identifying\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37b8e9",
   "metadata": {},
   "source": [
    "##### üö®‚ö†Ô∏è As you have noted, for the test query we provided, the answer from the **same LLM** is **completely wrong** ‚ùåü§Ø‚ÄºÔ∏è\n",
    "\n",
    "> üí¨ It confidently gives a **wrong answer** ‚Äî showing **why refinement and retrieval matter** so much in real-world usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4f0f297",
   "metadata": {},
   "source": [
    "#### ‚û°Ô∏è **For further information**:\n",
    "##### üìò Introduction to FlareChain in LangChain\n",
    "\n",
    "**FlareChain** is an advanced chain in the LangChain framework üß†‚öôÔ∏è designed to *iteratively refine answers* from a language model. It improves response quality by:\n",
    "\n",
    "üîç Identifying **low-confidence** spans  \n",
    "‚ùì Generating **clarifying questions**  \n",
    "üìö Retrieving **relevant context**  \n",
    "üîÅ Updating the answer in a loop\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Key Arguments of `FlareChain`\n",
    "\n",
    "##### üó£ 2. `response_chain`\n",
    "Generates the actual response using user input + context.\n",
    "\n",
    "##### üßæ 3. `output_parser`\n",
    "Checks whether the current answer is ‚Äúgood enough‚Äù to stop refinement.\n",
    "\n",
    "##### üì° 4. `retriever`\n",
    "Fetches documents to provide factual backup for refining the answer.\n",
    "\n",
    "##### üìâ 5. `min_prob`\n",
    "Low-confidence threshold (default: `0.2`) ‚Äì tokens below this are flagged for review.\n",
    "\n",
    "##### ‚ÜîÔ∏è 6. `min_token_gap`\n",
    "Ensures separation between two flagged spans (default: `5` tokens).\n",
    "\n",
    "##### üß∑ 7. `num_pad_tokens`\n",
    "Adds context tokens around flagged spans (default: `2`).\n",
    "\n",
    "##### üîÅ 8. `max_iter`\n",
    "Max number of refinement cycles (default: `10`).\n",
    "\n",
    "##### üß≠ 9. `start_with_retrieval`\n",
    "If `True`, starts by retrieving context even before generating the first draft.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßæ Inputs and Outputs\n",
    "\n",
    "- üì• **Input Key**: `user_input`  \n",
    "- üì§ **Output Key**: `response`  \n",
    "\n",
    "üí° The chain processes a single user prompt and returns an *improved, confident, and context-aware response*.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìö References\n",
    "\n",
    "- [LangChain FlareChain Documentation](https://api.python.langchain.com/en/latest/langchain/chains/langchain.chains.flare.base.FlareChain.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadd022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
